{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5gCg6jeUuWz"
      },
      "source": [
        "# **TUBES NLP**\n",
        "### **Neural Machine Translation with Seq2Seq Architecture (Engâ†’Ina)**\n",
        "### **Menggunakan GRU - max sentence length = 10**\n",
        "\n",
        "#### Ruhiyah Faradishi Widiaputri\n",
        "#### 13519034\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0irVaMk1WiuC"
      },
      "source": [
        "# IMPORT NEEDED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Zh_4LHGX_u"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rux89uvWmmX",
        "outputId": "4aa2e4ef-1022-4e87-fdaa-d83ba39bb7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZkK9xikVSG0"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7gAY7aPVW4D"
      },
      "source": [
        "This NMT trains with ... dataset from IndoNLG ([https://github.com/IndoNLP/indonlg](https://github.com/IndoNLP/indonlg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAYKsKbOUtzx",
        "outputId": "17bf0cd9-cd34-4813-ebae-c1bb5a8e145c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Tahun 4/NLP/tubes-mt/MT_TED_MULTI\n"
          ]
        }
      ],
      "source": [
        "# read train data\n",
        "%cd /content/drive/My Drive/Tahun 4/NLP/tubes-mt/MT_TED_MULTI/\n",
        "train_data_dir = \"train_preprocess.json\"\n",
        "val_data_dir = 'valid_preprocess.json'\n",
        "test_data_dir = 'test_preprocess.json'\n",
        "trained_model_encoder_path = \"seq2seq_2/trained_model_encoder_93.3\"\n",
        "trained_model_decoder_path = \"seq2seq_2/trained_model_decoder_93.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnhXf97_GtM0"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0he7C9RrxoLc",
        "outputId": "6c9e16a1-4fcb-418d-a798-402a0a0bec1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 87406 sentence pairs\n",
            "Trimmed to 19230 sentence pairs\n",
            "Counted words:\n",
            "en 10196\n",
            "ina 9916\n"
          ]
        }
      ],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "def normalize_string(s):\n",
        "  s = s.lower()\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "  return s\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def load_data(filename):\n",
        "  f = open(filename)\n",
        "  json_data = json.load(f)\n",
        "  data = []\n",
        "  for j in json_data:\n",
        "    text = normalize_string(j[\"text\"])\n",
        "    label = normalize_string(j[\"label\"])\n",
        "    data.append([text, label])\n",
        "  return data\n",
        "\n",
        "# define input and output lang\n",
        "input_lang = Lang(\"en\")\n",
        "output_lang = Lang(\"ina\")\n",
        "\n",
        "# load + normalize train data\n",
        "train_data = load_data(train_data_dir)\n",
        "\n",
        "# check how many sentence pairs\n",
        "print(\"Read %s sentence pairs\" % len(train_data))\n",
        "\n",
        "# take only data train with len < 20\n",
        "train_data = filterPairs(train_data)\n",
        "print(\"Trimmed to %s sentence pairs\" % len(train_data))\n",
        "  \n",
        "# add vocabulary\n",
        "for tr in train_data:\n",
        "  input_lang.addSentence(tr[0])\n",
        "  output_lang.addSentence(tr[1])\n",
        "\n",
        "print(\"Counted words:\")\n",
        "print(input_lang.name, input_lang.n_words)\n",
        "print(output_lang.name, output_lang.n_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSVbh6aQGIp8"
      },
      "source": [
        "# THE SEQ2SEQ MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "8p-hTN6UQ4EW",
        "outputId": "c9a88756-6b00-4251-f76d-c0a52504bd4a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n***** LAYER-LAYER YANG DIPAKAI *****\\n  # Embedding layer\\n    - CLASS\\n      torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, \\n      norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\\n    - Tabel lookup yang menyimpan embedding\\n    - ukuran vocab (num_embeddings) dan dimensi setiap vektor embedding (embedding_dim) tetap\\n    - pada dasarnya sama seperti linear layer tapi dia melakukan lookup instead of matrix-vector multiplication\\n      - The embedding weights and the linear layers weights are transposed to each other.\\n      - The embedding requires the sum(0)\\n    - masukan:\\n      - IntTensor atau LongTensor yang mengandung indeks-indeks yang akan diekstrak\\n    - keluaran: (shape input, embedding_dim)\\n  # GRU layer\\n    - CLASS:\\n      torch.nn.GRU(*args, **kwargs)\\n    - parameter-parameter:\\n        - input_size : banyak fitur di input x\\n        - hidden_size : banyak fitur hidden state h\\n        - num_layers : banyak layer (stack)\\n        - bias\\n        - batch_first\\n        - dropout\\n        - bidirectional\\n    - masukan: inputs, h_0\\n        - inputs: tensor dengan ukuran :\\n          - input tanpa batch : (sequence_length, input_size)\\n          - input dengan batch, batch_first=False (default) : (sequence_length, batch size, input_size)\\n          - input dengan batch, batch_first=True : (batch size, sequence_length, input_size)\\n        - h_0: tensor dengan ukuran :\\n          - (num_direction * num_layers, hidden_size), atau \\n          - (num_direction * num_layers, batch_size, hidden_size)\\n    - keluaran: output, h_n\\n        - output: tensor dengan ukuran :\\n          - tanpa batch : (sequence_length, num_direction*hidden_size)\\n          - batch_first=False : (sequence_length, batch_size, num_direction*hidden_size)\\n          - batch_first=True : (batch_size, sequence_length, num_direction*hidden_size)\\n        - h_n: tensor dengan ukuran :\\n          - (num_direction*num_layers, hidden_size), atau\\n          - (num_direction*num_layers, batch_size, hidden_size)\\n  # Linear layer\\n    - CLASS:\\n      torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\\n    - parameter-parameter:\\n      - in_features(int) : ukuran setiap sampel input\\n      - out_features(int) : ukuran setiap sampel output\\n      - bias(bool): ada bias atau tidak\\n  # Dropout layer\\n    - CLASS:\\n      torch.nn.Dropout(p=0.5, inplace=False)\\n    - selama training layer ini akan mengubah nilai beberapa elemen dari tensor input menjadi nol \\n      secara random dengan probabilitas=p menggunakan sampel dari distribusi Bernoulli\\n    - terbukti menjadi teknik yang efektif untuk regularisasi (dapat mencegah underfitting atau overfitting) dan mencegah ko-adaptasi neuron\\n\\n***** Fungsi-fungsi yang Dipakai *****\\n  # Softmax\\n    - torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\\n    - parameter-parameter\\n      - input(Tensor) : input\\n      - dim(int): dimensi di mana softmax akan dihitung\\n      - dtype(torch.dtype, optional): tipe data tensor keluaran yang diinginkan (sebelum dioperasikan tensor input dicasting ke dtype ini)\\n    - return type: Tensor\\n  # relu\\n    - torch.nn.functional.relu(input, inplace=False)\\n    - element-wise fungsi ReLU: max(0,input)\\n  # log_softmax\\n    - torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\\n    - log(softmax(x))\\n    - parameter sama seperti softmax\\n  # cat\\n    - torch.cat(tensors, dim=0, *, out=None)\\n    - mengkonkatenasi sekuens tensor dalam dimensi tertentu\\n    - parameter-parameter:\\n      - tensors(sequence of Tensors)\\n      - dim (int, optional); cth untuk tensor 2 dimensi:\\n        - dim = 0 --> konkatenasi baris\\n        - dim = 1 --> konkatenasi kolom\\n  # unsqueeze\\n    - mengubah dimensi tensor dari n menjadi n+1\\n    - parameter dim untuk menentukan sumbu mana dimensi baru harus berada\\n      berturut-turut dari luar ke lebih dalam\\n  # squeeze\\n    - torch.squeeze(input, dim=None)\\n    - mengembalikan tensor dengan dimensi input ukuran 1 dihapus\\n    - jika nilai dim diberikan dan di dimensi ke-dim itu sizenya=1 maka akan disqueeze di dim itu saja\\n  # bmm\\n    - torch.bmm(input, mat2, *, out=None)\\n    - melakukan perkalian batch matriks-matriks input dan mat2\\n    - input dan mat2 harus tensor 3D\\n    - jika input=tensor(bxnxm), mat2=tensor(bxmxp) maka out=tensor(bxnxp) \\n  # topk\\n    - torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)\\n    - mengembalikan k elemen terbesar (atau terkecil jika largest diset False) dari input yang diberikan pada dimensi yang diberikan\\n    - jika dim tidak diset diambil dimensi paling terakhir\\n  # detach\\n    - Tensor.detach()\\n    - mengembalikan tensor baru yang hasilnya tidak membutuhkan gradien\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "***** LAYER-LAYER YANG DIPAKAI *****\n",
        "  # Embedding layer\n",
        "    - CLASS\n",
        "      torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, \n",
        "      norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\n",
        "    - Tabel lookup yang menyimpan embedding\n",
        "    - ukuran vocab (num_embeddings) dan dimensi setiap vektor embedding (embedding_dim) tetap\n",
        "    - pada dasarnya sama seperti linear layer tapi dia melakukan lookup instead of matrix-vector multiplication\n",
        "      - The embedding weights and the linear layers weights are transposed to each other.\n",
        "      - The embedding requires the sum(0)\n",
        "    - masukan:\n",
        "      - IntTensor atau LongTensor yang mengandung indeks-indeks yang akan diekstrak\n",
        "    - keluaran: (shape input, embedding_dim)\n",
        "  # GRU layer\n",
        "    - CLASS:\n",
        "      torch.nn.GRU(*args, **kwargs)\n",
        "    - parameter-parameter:\n",
        "        - input_size : banyak fitur di input x\n",
        "        - hidden_size : banyak fitur hidden state h\n",
        "        - num_layers : banyak layer (stack)\n",
        "        - bias\n",
        "        - batch_first\n",
        "        - dropout\n",
        "        - bidirectional\n",
        "    - masukan: inputs, h_0\n",
        "        - inputs: tensor dengan ukuran :\n",
        "          - input tanpa batch : (sequence_length, input_size)\n",
        "          - input dengan batch, batch_first=False (default) : (sequence_length, batch size, input_size)\n",
        "          - input dengan batch, batch_first=True : (batch size, sequence_length, input_size)\n",
        "        - h_0: tensor dengan ukuran :\n",
        "          - (num_direction * num_layers, hidden_size), atau \n",
        "          - (num_direction * num_layers, batch_size, hidden_size)\n",
        "    - keluaran: output, h_n\n",
        "        - output: tensor dengan ukuran :\n",
        "          - tanpa batch : (sequence_length, num_direction*hidden_size)\n",
        "          - batch_first=False : (sequence_length, batch_size, num_direction*hidden_size)\n",
        "          - batch_first=True : (batch_size, sequence_length, num_direction*hidden_size)\n",
        "        - h_n: tensor dengan ukuran :\n",
        "          - (num_direction*num_layers, hidden_size), atau\n",
        "          - (num_direction*num_layers, batch_size, hidden_size)\n",
        "  # Linear layer\n",
        "    - CLASS:\n",
        "      torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
        "    - parameter-parameter:\n",
        "      - in_features(int) : ukuran setiap sampel input\n",
        "      - out_features(int) : ukuran setiap sampel output\n",
        "      - bias(bool): ada bias atau tidak\n",
        "  # Dropout layer\n",
        "    - CLASS:\n",
        "      torch.nn.Dropout(p=0.5, inplace=False)\n",
        "    - selama training layer ini akan mengubah nilai beberapa elemen dari tensor input menjadi nol \n",
        "      secara random dengan probabilitas=p menggunakan sampel dari distribusi Bernoulli\n",
        "    - terbukti menjadi teknik yang efektif untuk regularisasi (dapat mencegah underfitting atau overfitting) dan mencegah ko-adaptasi neuron\n",
        "\n",
        "***** Fungsi-fungsi yang Dipakai *****\n",
        "  # Softmax\n",
        "    - torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
        "    - parameter-parameter\n",
        "      - input(Tensor) : input\n",
        "      - dim(int): dimensi di mana softmax akan dihitung\n",
        "      - dtype(torch.dtype, optional): tipe data tensor keluaran yang diinginkan (sebelum dioperasikan tensor input dicasting ke dtype ini)\n",
        "    - return type: Tensor\n",
        "  # relu\n",
        "    - torch.nn.functional.relu(input, inplace=False)\n",
        "    - element-wise fungsi ReLU: max(0,input)\n",
        "  # log_softmax\n",
        "    - torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
        "    - log(softmax(x))\n",
        "    - parameter sama seperti softmax\n",
        "  # cat\n",
        "    - torch.cat(tensors, dim=0, *, out=None)\n",
        "    - mengkonkatenasi sekuens tensor dalam dimensi tertentu\n",
        "    - parameter-parameter:\n",
        "      - tensors(sequence of Tensors)\n",
        "      - dim (int, optional); cth untuk tensor 2 dimensi:\n",
        "        - dim = 0 --> konkatenasi baris\n",
        "        - dim = 1 --> konkatenasi kolom\n",
        "  # unsqueeze\n",
        "    - mengubah dimensi tensor dari n menjadi n+1\n",
        "    - parameter dim untuk menentukan sumbu mana dimensi baru harus berada\n",
        "      berturut-turut dari luar ke lebih dalam\n",
        "  # squeeze\n",
        "    - torch.squeeze(input, dim=None)\n",
        "    - mengembalikan tensor dengan dimensi input ukuran 1 dihapus\n",
        "    - jika nilai dim diberikan dan di dimensi ke-dim itu sizenya=1 maka akan disqueeze di dim itu saja\n",
        "  # bmm\n",
        "    - torch.bmm(input, mat2, *, out=None)\n",
        "    - melakukan perkalian batch matriks-matriks input dan mat2\n",
        "    - input dan mat2 harus tensor 3D\n",
        "    - jika input=tensor(bxnxm), mat2=tensor(bxmxp) maka out=tensor(bxnxp) \n",
        "  # topk\n",
        "    - torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)\n",
        "    - mengembalikan k elemen terbesar (atau terkecil jika largest diset False) dari input yang diberikan pada dimensi yang diberikan\n",
        "    - jika dim tidak diset diambil dimensi paling terakhir\n",
        "  # detach\n",
        "    - Tensor.detach()\n",
        "    - mengembalikan tensor baru yang hasilnya tidak membutuhkan gradien\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR_MQpz3GPV5"
      },
      "outputs": [],
      "source": [
        "# encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size                            \n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "WEDf7HOUzpmr",
        "outputId": "63411a49-fd61-4025-cee3-e44e40484f47"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n=== ENCODER ===\\nTerdiri dari :\\n- 1 embedding layer\\n- 1 GRU layer\\n\\nForward:\\n  1. input masuk ke embedding layer\\n  2. hasil (1) diubah jadi berdimensi (1x1x_)\\n  3. hasil (2) dan hidden masukkan ke layer GRU\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "=== ENCODER ===\n",
        "Terdiri dari :\n",
        "- 1 embedding layer\n",
        "- 1 GRU layer\n",
        "\n",
        "Forward:\n",
        "  1. input masuk ke embedding layer\n",
        "  2. hasil (1) diubah jadi berdimensi (1x1x_)\n",
        "  3. hasil (2) dan hidden masukkan ke layer GRU\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCudtmIJGmdS"
      },
      "outputs": [],
      "source": [
        "# decoder - with attention mechanism\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "xxpbEWxjI_gT",
        "outputId": "e92de26e-55f0-40f8-9130-199377ee27eb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n=== DECODER ===\\nMenggunakan mekanisme atensi\\nTerdiri dari:\\n- 1 embedding layer\\n- 3 linear layer\\n- 1 dropout layer\\n- 1 gru layer\\n\\nForward:\\n  1. input masuk ke embedding layer\\n  2. hasil (1) diubah jadi berdimensi (1x1x_)\\n  3. hasil (2) masuk ke dropout layer\\n  3. embedded[0] dan hidden[0] dikonkatenasi\\n  4. hasil (3) dimasukkan ke linear layer\\n  5. hasil (4) diambil softmax --> jadi attention weight \\n  6. hasil (5) dikenakan fungsi unsqueeze\\n  7. output encoder dikenakan fungsi unsqueeze\\n  8. hasil (6) dan (7) dikenakan fungsi bmm\\n  9. embedded[0] dan hasil (8) dikonkatenasi\\n  10. hasil (9) masuk ke linear layer\\n  11. hasil (10) dikenakan fungsi unsqueeze\\n  12. hasil (11) dikenakan fungsi relu\\n  13. hasil (12) masuk ke layer GRU\\n  14. hasil (13) masuk ke linear layer\\n  15. hasil (14) dikenakan fungsi log_softmax\\n\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "=== DECODER ===\n",
        "Menggunakan mekanisme atensi\n",
        "Terdiri dari:\n",
        "- 1 embedding layer\n",
        "- 3 linear layer\n",
        "- 1 dropout layer\n",
        "- 1 gru layer\n",
        "\n",
        "Forward:\n",
        "  1. input masuk ke embedding layer\n",
        "  2. hasil (1) diubah jadi berdimensi (1x1x_)\n",
        "  3. hasil (2) masuk ke dropout layer\n",
        "  3. embedded[0] dan hidden[0] dikonkatenasi\n",
        "  4. hasil (3) dimasukkan ke linear layer\n",
        "  5. hasil (4) diambil softmax --> jadi attention weight \n",
        "  6. hasil (5) dikenakan fungsi unsqueeze\n",
        "  7. output encoder dikenakan fungsi unsqueeze\n",
        "  8. hasil (6) dan (7) dikenakan fungsi bmm\n",
        "  9. embedded[0] dan hasil (8) dikonkatenasi\n",
        "  10. hasil (9) masuk ke linear layer\n",
        "  11. hasil (10) dikenakan fungsi unsqueeze\n",
        "  12. hasil (11) dikenakan fungsi relu\n",
        "  13. hasil (12) masuk ke layer GRU\n",
        "  14. hasil (13) masuk ke linear layer\n",
        "  15. hasil (14) dikenakan fungsi log_softmax\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxj6ozMPG0D3"
      },
      "source": [
        "# TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOtDvRV8GyQm"
      },
      "outputs": [],
      "source": [
        "# preparing training data\n",
        "\n",
        "# mengambil indeks dari setiap kata di sentence --> hasilnya list of indeks kata\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index else 0 for word in sentence.split(' ')]\n",
        "\n",
        "# mengembalikan tensor dengan elemennya adalah list of indeks kata yang dikonkatenasi dengan EOS_TOKEN (1)\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# mengembalikan tuple (input_tensor, target_tensor)\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzJteTQUG7M0"
      },
      "outputs": [],
      "source": [
        "# program for training\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    # inisialisasi hidden state encoder dengan zeros(1x1xhidden_size)\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    # set gradien semua tensor yang dioptimasi jadi nol : supaya tidak terakumulasi dengan gradien yang sudah ada\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # hitung panjang input dan output\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # inisialisasi encoder_outputs dengan zeros(max_length x hidden_size)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    # inisialisasi nilai loss = 0\n",
        "    loss = 0\n",
        "\n",
        "    # untuk setiap input lakukan:\n",
        "    for ei in range(input_length):\n",
        "        # forward encoder\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        # update nilai encoder_outputs\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    # inisialisasi input decoder dengan tensor [[0]]\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    # inisialisasi nilai hidden state decoder menjadi sama dengan hidden state encoder\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # dengan peluang teacher_forcing_ratio, tentukan apakah akan memakai teacher forcing atau tidak\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # jika memakai teacher forcing: \n",
        "    if use_teacher_forcing:\n",
        "        # untuk setiap output lakukan:\n",
        "        for di in range(target_length):\n",
        "            # forward decoder\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # update loss\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            # teacher forcing: Feed the target as the next input\n",
        "            decoder_input = target_tensor[di]  \n",
        "\n",
        "    # jika tidak menggunakan teacher forcing: use its own predictions as the next input\n",
        "    else:\n",
        "        # untuk setiap output lakukan:\n",
        "        for di in range(target_length):\n",
        "            # forward decoder\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # ambil nilai decoder_output terbesar\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            # lepaskan dari histori sebagai input\n",
        "            decoder_input = topi.squeeze().detach()  \n",
        "            # update loss\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            # berhenti jika sudah menemukan token EOS\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "    # lakukan backpropagation\n",
        "    loss.backward()\n",
        "    \n",
        "    # inisiasi optimizer\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj5c538mHEew"
      },
      "outputs": [],
      "source": [
        "# helper\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYlYpgIEHISo"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    # hitung waktu mulai\n",
        "    start = time.time()\n",
        "\n",
        "    plot_losses = []\n",
        "\n",
        "    # Reset every print_every\n",
        "    print_loss_total = 0 \n",
        "    # Reset every plot_every \n",
        "    plot_loss_total = 0  \n",
        "\n",
        "    # encoder menggunakan optimizer SGD (stochastic gradient descent)\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    # decoder menggunakan optimizer SGD (stochastic gradient descent)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # mengambil list of pasangan di train data secara acak sebanyak n_iters\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_data))\n",
        "                      for i in range(n_iters)]\n",
        "\n",
        "    # menggunakan loss negative log likelihood\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # untuk setiap pasangan di training_pairs\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        # lakukan training\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        \n",
        "        # update nilai loss total\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        # untuk menampilkan : jika sudah iterasi kelipatan print_every \n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            perc = iter / n_iters * 100\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, perc , print_loss_avg))\n",
        "            # save setiap iterasi kelipatan print_every\n",
        "            torch.save(encoder1.state_dict(), f\"{trained_model_encoder_path}_{perc}.pt\")\n",
        "            torch.save(attn_decoder1.state_dict(), f\"{trained_model_decoder_path}_{perc}.pt\")\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW4w4_HDHE_p"
      },
      "source": [
        "## TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBp8MsAZHNut",
        "outputId": "102537d3-7747-4dd9-dc6d-e3d3cfada844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Time = 13:16:35\n",
            "13m 55s (- 195m 0s) (5000 6%) 4.7653\n",
            "27m 56s (- 181m 34s) (10000 13%) 4.3873\n",
            "41m 54s (- 167m 38s) (15000 20%) 4.1583\n",
            "56m 3s (- 154m 8s) (20000 26%) 3.9904\n",
            "70m 13s (- 140m 27s) (25000 33%) 3.7716\n",
            "84m 9s (- 126m 13s) (30000 40%) 3.6400\n",
            "98m 7s (- 112m 8s) (35000 46%) 3.5802\n",
            "112m 20s (- 98m 18s) (40000 53%) 3.4407\n",
            "126m 42s (- 84m 28s) (45000 60%) 3.3341\n",
            "141m 18s (- 70m 39s) (50000 66%) 3.2739\n",
            "155m 46s (- 56m 38s) (55000 73%) 3.2249\n",
            "170m 11s (- 42m 32s) (60000 80%) 3.1286\n",
            "184m 31s (- 28m 23s) (65000 86%) 3.0099\n",
            "199m 48s (- 14m 16s) (70000 93%) 2.9737\n"
          ]
        }
      ],
      "source": [
        "# current time\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print(\"Start Time =\", current_time)\n",
        "\n",
        "# train\n",
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "#trainIters(encoder1, attn_decoder1, 2, print_every=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4vEfk8BKsOO"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "finish = datetime.now()\n",
        "\n",
        "finish_time = finish.strftime(\"%H:%M:%S\")\n",
        "print(\"Finish Time =\", finish_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_N1RapUG-eB"
      },
      "outputs": [],
      "source": [
        "# save trained model - state dict\n",
        "torch.save(encoder1.state_dict(), f\"{trained_model_encoder_path}.pt\")\n",
        "torch.save(attn_decoder1.state_dict(), f\"{trained_model_decoder_path}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVi-V3jszCwU"
      },
      "source": [
        "# LOAD TRAINED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0mgAx8qzEWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165e7c0f-e1cf-4c90-ea8f-c2061ff8af4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderRNN(\n",
              "  (embedding): Embedding(10196, 256)\n",
              "  (gru): GRU(256, 256)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# redefine the model\n",
        "hidden_size = 256\n",
        "encoder_trained = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder_trained = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "# load encoder trained model\n",
        "encoder_trained.load_state_dict(torch.load(f\"{trained_model_encoder_path}.pt\"))\n",
        "encoder_trained.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3olx8xO1fIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0babd4-e11d-451a-da4f-87cbddf1fe40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttnDecoderRNN(\n",
              "  (embedding): Embedding(9916, 256)\n",
              "  (attn): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (gru): GRU(256, 256)\n",
              "  (out): Linear(in_features=256, out_features=9916, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# load decoder trained model\n",
        "attn_decoder_trained.load_state_dict(torch.load(f\"{trained_model_decoder_path}.pt\"))\n",
        "attn_decoder_trained.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkWhJpoc2DFZ"
      },
      "source": [
        "# EVALUATE THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load validation data\n",
        "# load + normalize train data\n",
        "val_data = load_data(val_data_dir)\n",
        "\n",
        "# check how many sentence pairs\n",
        "print(\"Read %s sentence pairs\" % len(val_data))\n",
        "\n",
        "# take only data train with len < 20\n",
        "val_data = filterPairs(val_data)\n",
        "print(\"Trimmed to %s sentence pairs\" % len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBqzYaCvbFYD",
        "outputId": "e81eaf39-c8d0-4e53-c9a1-dafa076fed71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 2677 sentence pairs\n",
            "Trimmed to 618 sentence pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIY7gbZJ1iTN"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, encoder, decoder, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = datum[0]\n",
        "        trg = [o for o in datum[1].split(' ')]\n",
        "        \n",
        "        output_words, attentions = evaluate(encoder, decoder, src)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = output_words[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "metadata": {
        "id": "fdP3JlAQbMtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scr = calculate_bleu(val_data, encoder_trained, attn_decoder_trained, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_scr*100:.10f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nGpmMsYbPV0",
        "outputId": "e14cd034-2e69-428c-f632-2391003396dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 8.6885338522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r05QgcUh3Ljs"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(val_data)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yMGMo5D3Pls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bd9689-e7e7-4ade-f351-82eb25dc3ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> so i went to a library .\n",
            "= jadi saya pergi ke perpustakaan .\n",
            "< saya pergi ke pergi . <EOS>\n",
            "\n",
            "> think about it this way proportionately .\n",
            "= coba bayangkan seperti ini secara proporsional .\n",
            "< pikirkan dengan cara kerja ini . <EOS>\n",
            "\n",
            "> i had never seen an airplane before .\n",
            "= saya tidak pernah melihat pesawat terbang sebelumnya .\n",
            "< saya tidak pernah pernah menjadi . <EOS>\n",
            "\n",
            "> and this can get really dangerous .\n",
            "= dan ini bisa menjadi sesuatu yang sangat berbahaya .\n",
            "< dan ini sangat benar sangat . . <EOS>\n",
            "\n",
            "> and i also thought what a waste .\n",
            "= dan saya juga terpikir sangat disayangkan .\n",
            "< dan saya juga bagaimana sebuah . <EOS>\n",
            "\n",
            "> look what happens when we keep going .\n",
            "= lihat apa yang terjadi kalau kita lanjutkan .\n",
            "< kita tahu apa yang kami . <EOS>\n",
            "\n",
            "> thank you and peace and blessings .\n",
            "= terima kasih salam damai dan penuh berkah .\n",
            "< terima kasih dan mendengarkan dan . <EOS>\n",
            "\n",
            "> so there is no mistake .\n",
            "= jadi tidak ada kesalahan .\n",
            "< jadi tidak ada ada . <EOS>\n",
            "\n",
            "> this is a problem we have to solve .\n",
            "= ini masalah yang harus bisa kita selesaikan .\n",
            "< ini adalah masalah kita harus harus . <EOS>\n",
            "\n",
            "> i had never used a computer .\n",
            "= saya tidak pernah menggunakan komputer .\n",
            "< saya tidak dapat menemukan yang . . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder_trained, attn_decoder_trained)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRY THIS MODEL TO LAPTOP REVIEW SUMMARY RESULT"
      ],
      "metadata": {
        "id": "loVvYwuPNg1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_fast(input_sent, encoder=encoder_trained, decoder=attn_decoder_trained):\n",
        "  output_words, attentions = evaluate(encoder, decoder, input_sent)\n",
        "  output_sentence = ' '.join(output_words)\n",
        "  print(f\"> {output_sentence}\") \n",
        "\n",
        "translate_fast(\"great product .\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_y8acBiNp58",
        "outputId": "e286fc75-60e9-4ff4-9e8b-0a7ee69c8862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> bagus bagus . <EOS>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}