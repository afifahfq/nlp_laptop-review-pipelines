{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5gCg6jeUuWz"
      },
      "source": [
        "# **TUBES NLP**\n",
        "### **Neural Machine Translation with Seq2Seq Architecture (Engâ†’Ina)**\n",
        "### **Menggunakan GRU : max sentence length = 15**\n",
        "\n",
        "#### Ruhiyah Faradishi Widiaputri\n",
        "#### 13519034\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0irVaMk1WiuC"
      },
      "source": [
        "# IMPORT NEEDED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Zh_4LHGX_u"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rux89uvWmmX",
        "outputId": "a3474688-f50a-4f82-982a-7f305409596d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZkK9xikVSG0"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7gAY7aPVW4D"
      },
      "source": [
        "This NMT trains with ... dataset from IndoNLG ([https://github.com/IndoNLP/indonlg](https://github.com/IndoNLP/indonlg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAYKsKbOUtzx",
        "outputId": "23e1c5c3-db6d-4d81-b19b-4b1f8867da37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Tahun 4/NLP/tubes-mt/MT_TED_MULTI\n"
          ]
        }
      ],
      "source": [
        "# read train data\n",
        "%cd /content/drive/My Drive/Tahun 4/NLP/tubes-mt/MT_TED_MULTI/\n",
        "train_data_dir = \"train_preprocess.json\"\n",
        "val_data_dir = 'valid_preprocess.json'\n",
        "test_data_dir = 'test_preprocess.json'\n",
        "trained_model_encoder_path = \"seq2seq_1/trained_model_encoder\"\n",
        "trained_model_decoder_path = \"seq2seq_1/trained_model_decoder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnhXf97_GtM0"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0he7C9RrxoLc",
        "outputId": "58a4b2ce-953a-4c57-dc47-387985df91c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 87406 sentence pairs\n",
            "Trimmed to 40063 sentence pairs\n",
            "Counted words:\n",
            "en 17725\n",
            "ina 16806\n"
          ]
        }
      ],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "def normalize_string(s):\n",
        "  s = s.lower()\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "  return s\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def load_data(filename):\n",
        "  f = open(filename)\n",
        "  json_data = json.load(f)\n",
        "  data = []\n",
        "  for j in json_data:\n",
        "    text = normalize_string(j[\"text\"])\n",
        "    label = normalize_string(j[\"label\"])\n",
        "    data.append([text, label])\n",
        "  return data\n",
        "\n",
        "# define input and output lang\n",
        "input_lang = Lang(\"en\")\n",
        "output_lang = Lang(\"ina\")\n",
        "\n",
        "# load + normalize train data\n",
        "train_data = load_data(train_data_dir)\n",
        "\n",
        "# check how many sentence pairs\n",
        "print(\"Read %s sentence pairs\" % len(train_data))\n",
        "\n",
        "# take only data train with len < 20\n",
        "train_data = filterPairs(train_data)\n",
        "print(\"Trimmed to %s sentence pairs\" % len(train_data))\n",
        "  \n",
        "# add vocabulary\n",
        "for tr in train_data:\n",
        "  input_lang.addSentence(tr[0])\n",
        "  output_lang.addSentence(tr[1])\n",
        "\n",
        "print(\"Counted words:\")\n",
        "print(input_lang.name, input_lang.n_words)\n",
        "print(output_lang.name, output_lang.n_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSVbh6aQGIp8"
      },
      "source": [
        "# THE SEQ2SEQ MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "8p-hTN6UQ4EW",
        "outputId": "a84daf39-1406-43e7-b5d3-30776acf8ce9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n***** LAYER-LAYER YANG DIPAKAI *****\\n  # Embedding layer\\n    - CLASS\\n      torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, \\n      norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\\n    - Tabel lookup yang menyimpan embedding\\n    - ukuran vocab (num_embeddings) dan dimensi setiap vektor embedding (embedding_dim) tetap\\n    - pada dasarnya sama seperti linear layer tapi dia melakukan lookup instead of matrix-vector multiplication\\n      - The embedding weights and the linear layers weights are transposed to each other.\\n      - The embedding requires the sum(0)\\n    - masukan:\\n      - IntTensor atau LongTensor yang mengandung indeks-indeks yang akan diekstrak\\n    - keluaran: (shape input, embedding_dim)\\n  # GRU layer\\n    - CLASS:\\n      torch.nn.GRU(*args, **kwargs)\\n    - parameter-parameter:\\n        - input_size : banyak fitur di input x\\n        - hidden_size : banyak fitur hidden state h\\n        - num_layers : banyak layer (stack)\\n        - bias\\n        - batch_first\\n        - dropout\\n        - bidirectional\\n    - masukan: inputs, h_0\\n        - inputs: tensor dengan ukuran :\\n          - input tanpa batch : (sequence_length, input_size)\\n          - input dengan batch, batch_first=False (default) : (sequence_length, batch size, input_size)\\n          - input dengan batch, batch_first=True : (batch size, sequence_length, input_size)\\n        - h_0: tensor dengan ukuran :\\n          - (num_direction * num_layers, hidden_size), atau \\n          - (num_direction * num_layers, batch_size, hidden_size)\\n    - keluaran: output, h_n\\n        - output: tensor dengan ukuran :\\n          - tanpa batch : (sequence_length, num_direction*hidden_size)\\n          - batch_first=False : (sequence_length, batch_size, num_direction*hidden_size)\\n          - batch_first=True : (batch_size, sequence_length, num_direction*hidden_size)\\n        - h_n: tensor dengan ukuran :\\n          - (num_direction*num_layers, hidden_size), atau\\n          - (num_direction*num_layers, batch_size, hidden_size)\\n  # Linear layer\\n    - CLASS:\\n      torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\\n    - parameter-parameter:\\n      - in_features(int) : ukuran setiap sampel input\\n      - out_features(int) : ukuran setiap sampel output\\n      - bias(bool): ada bias atau tidak\\n  # Dropout layer\\n    - CLASS:\\n      torch.nn.Dropout(p=0.5, inplace=False)\\n    - selama training layer ini akan mengubah nilai beberapa elemen dari tensor input menjadi nol \\n      secara random dengan probabilitas=p menggunakan sampel dari distribusi Bernoulli\\n    - terbukti menjadi teknik yang efektif untuk regularisasi (dapat mencegah underfitting atau overfitting) dan mencegah ko-adaptasi neuron\\n\\n***** Fungsi-fungsi yang Dipakai *****\\n  # Softmax\\n    - torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\\n    - parameter-parameter\\n      - input(Tensor) : input\\n      - dim(int): dimensi di mana softmax akan dihitung\\n      - dtype(torch.dtype, optional): tipe data tensor keluaran yang diinginkan (sebelum dioperasikan tensor input dicasting ke dtype ini)\\n    - return type: Tensor\\n  # relu\\n    - torch.nn.functional.relu(input, inplace=False)\\n    - element-wise fungsi ReLU: max(0,input)\\n  # log_softmax\\n    - torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\\n    - log(softmax(x))\\n    - parameter sama seperti softmax\\n  # cat\\n    - torch.cat(tensors, dim=0, *, out=None)\\n    - mengkonkatenasi sekuens tensor dalam dimensi tertentu\\n    - parameter-parameter:\\n      - tensors(sequence of Tensors)\\n      - dim (int, optional); cth untuk tensor 2 dimensi:\\n        - dim = 0 --> konkatenasi baris\\n        - dim = 1 --> konkatenasi kolom\\n  # unsqueeze\\n    - mengubah dimensi tensor dari n menjadi n+1\\n    - parameter dim untuk menentukan sumbu mana dimensi baru harus berada\\n      berturut-turut dari luar ke lebih dalam\\n  # squeeze\\n    - torch.squeeze(input, dim=None)\\n    - mengembalikan tensor dengan dimensi input ukuran 1 dihapus\\n    - jika nilai dim diberikan dan di dimensi ke-dim itu sizenya=1 maka akan disqueeze di dim itu saja\\n  # bmm\\n    - torch.bmm(input, mat2, *, out=None)\\n    - melakukan perkalian batch matriks-matriks input dan mat2\\n    - input dan mat2 harus tensor 3D\\n    - jika input=tensor(bxnxm), mat2=tensor(bxmxp) maka out=tensor(bxnxp) \\n  # topk\\n    - torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)\\n    - mengembalikan k elemen terbesar (atau terkecil jika largest diset False) dari input yang diberikan pada dimensi yang diberikan\\n    - jika dim tidak diset diambil dimensi paling terakhir\\n  # detach\\n    - Tensor.detach()\\n    - mengembalikan tensor baru yang hasilnya tidak membutuhkan gradien\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"\n",
        "***** LAYER-LAYER YANG DIPAKAI *****\n",
        "  # Embedding layer\n",
        "    - CLASS\n",
        "      torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, \n",
        "      norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\n",
        "    - Tabel lookup yang menyimpan embedding\n",
        "    - ukuran vocab (num_embeddings) dan dimensi setiap vektor embedding (embedding_dim) tetap\n",
        "    - pada dasarnya sama seperti linear layer tapi dia melakukan lookup instead of matrix-vector multiplication\n",
        "      - The embedding weights and the linear layers weights are transposed to each other.\n",
        "      - The embedding requires the sum(0)\n",
        "    - masukan:\n",
        "      - IntTensor atau LongTensor yang mengandung indeks-indeks yang akan diekstrak\n",
        "    - keluaran: (shape input, embedding_dim)\n",
        "  # GRU layer\n",
        "    - CLASS:\n",
        "      torch.nn.GRU(*args, **kwargs)\n",
        "    - parameter-parameter:\n",
        "        - input_size : banyak fitur di input x\n",
        "        - hidden_size : banyak fitur hidden state h\n",
        "        - num_layers : banyak layer (stack)\n",
        "        - bias\n",
        "        - batch_first\n",
        "        - dropout\n",
        "        - bidirectional\n",
        "    - masukan: inputs, h_0\n",
        "        - inputs: tensor dengan ukuran :\n",
        "          - input tanpa batch : (sequence_length, input_size)\n",
        "          - input dengan batch, batch_first=False (default) : (sequence_length, batch size, input_size)\n",
        "          - input dengan batch, batch_first=True : (batch size, sequence_length, input_size)\n",
        "        - h_0: tensor dengan ukuran :\n",
        "          - (num_direction * num_layers, hidden_size), atau \n",
        "          - (num_direction * num_layers, batch_size, hidden_size)\n",
        "    - keluaran: output, h_n\n",
        "        - output: tensor dengan ukuran :\n",
        "          - tanpa batch : (sequence_length, num_direction*hidden_size)\n",
        "          - batch_first=False : (sequence_length, batch_size, num_direction*hidden_size)\n",
        "          - batch_first=True : (batch_size, sequence_length, num_direction*hidden_size)\n",
        "        - h_n: tensor dengan ukuran :\n",
        "          - (num_direction*num_layers, hidden_size), atau\n",
        "          - (num_direction*num_layers, batch_size, hidden_size)\n",
        "  # Linear layer\n",
        "    - CLASS:\n",
        "      torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
        "    - parameter-parameter:\n",
        "      - in_features(int) : ukuran setiap sampel input\n",
        "      - out_features(int) : ukuran setiap sampel output\n",
        "      - bias(bool): ada bias atau tidak\n",
        "  # Dropout layer\n",
        "    - CLASS:\n",
        "      torch.nn.Dropout(p=0.5, inplace=False)\n",
        "    - selama training layer ini akan mengubah nilai beberapa elemen dari tensor input menjadi nol \n",
        "      secara random dengan probabilitas=p menggunakan sampel dari distribusi Bernoulli\n",
        "    - terbukti menjadi teknik yang efektif untuk regularisasi (dapat mencegah underfitting atau overfitting) dan mencegah ko-adaptasi neuron\n",
        "\n",
        "***** Fungsi-fungsi yang Dipakai *****\n",
        "  # Softmax\n",
        "    - torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
        "    - parameter-parameter\n",
        "      - input(Tensor) : input\n",
        "      - dim(int): dimensi di mana softmax akan dihitung\n",
        "      - dtype(torch.dtype, optional): tipe data tensor keluaran yang diinginkan (sebelum dioperasikan tensor input dicasting ke dtype ini)\n",
        "    - return type: Tensor\n",
        "  # relu\n",
        "    - torch.nn.functional.relu(input, inplace=False)\n",
        "    - element-wise fungsi ReLU: max(0,input)\n",
        "  # log_softmax\n",
        "    - torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
        "    - log(softmax(x))\n",
        "    - parameter sama seperti softmax\n",
        "  # cat\n",
        "    - torch.cat(tensors, dim=0, *, out=None)\n",
        "    - mengkonkatenasi sekuens tensor dalam dimensi tertentu\n",
        "    - parameter-parameter:\n",
        "      - tensors(sequence of Tensors)\n",
        "      - dim (int, optional); cth untuk tensor 2 dimensi:\n",
        "        - dim = 0 --> konkatenasi baris\n",
        "        - dim = 1 --> konkatenasi kolom\n",
        "  # unsqueeze\n",
        "    - mengubah dimensi tensor dari n menjadi n+1\n",
        "    - parameter dim untuk menentukan sumbu mana dimensi baru harus berada\n",
        "      berturut-turut dari luar ke lebih dalam\n",
        "  # squeeze\n",
        "    - torch.squeeze(input, dim=None)\n",
        "    - mengembalikan tensor dengan dimensi input ukuran 1 dihapus\n",
        "    - jika nilai dim diberikan dan di dimensi ke-dim itu sizenya=1 maka akan disqueeze di dim itu saja\n",
        "  # bmm\n",
        "    - torch.bmm(input, mat2, *, out=None)\n",
        "    - melakukan perkalian batch matriks-matriks input dan mat2\n",
        "    - input dan mat2 harus tensor 3D\n",
        "    - jika input=tensor(bxnxm), mat2=tensor(bxmxp) maka out=tensor(bxnxp) \n",
        "  # topk\n",
        "    - torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)\n",
        "    - mengembalikan k elemen terbesar (atau terkecil jika largest diset False) dari input yang diberikan pada dimensi yang diberikan\n",
        "    - jika dim tidak diset diambil dimensi paling terakhir\n",
        "  # detach\n",
        "    - Tensor.detach()\n",
        "    - mengembalikan tensor baru yang hasilnya tidak membutuhkan gradien\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR_MQpz3GPV5"
      },
      "outputs": [],
      "source": [
        "# encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size                            \n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WEDf7HOUzpmr",
        "outputId": "9e401266-c81b-4b30-bade-07128046a369"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n=== ENCODER ===\\nTerdiri dari :\\n- 1 embedding layer\\n- 1 GRU layer\\n\\nForward:\\n  1. input masuk ke embedding layer\\n  2. hasil (1) diubah jadi berdimensi (1x1x_)\\n  3. hasil (2) dan hidden masukkan ke layer GRU\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\"\"\"\n",
        "=== ENCODER ===\n",
        "Terdiri dari :\n",
        "- 1 embedding layer\n",
        "- 1 GRU layer\n",
        "\n",
        "Forward:\n",
        "  1. input masuk ke embedding layer\n",
        "  2. hasil (1) diubah jadi berdimensi (1x1x_)\n",
        "  3. hasil (2) dan hidden masukkan ke layer GRU\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCudtmIJGmdS"
      },
      "outputs": [],
      "source": [
        "# decoder - with attention mechanism\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "xxpbEWxjI_gT",
        "outputId": "f0cdd3d5-8418-49e4-e763-f26ee140c78d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n=== DECODER ===\\nMenggunakan mekanisme atensi\\nTerdiri dari:\\n- 1 embedding layer\\n- 3 linear layer\\n- 1 dropout layer\\n- 1 gru layer\\n\\nForward:\\n  1. input masuk ke embedding layer\\n  2. hasil (1) diubah jadi berdimensi (1x1x_)\\n  3. hasil (2) masuk ke dropout layer\\n  3. embedded[0] dan hidden[0] dikonkatenasi\\n  4. hasil (3) dimasukkan ke linear layer\\n  5. hasil (4) diambil softmax --> jadi attention weight \\n  6. hasil (5) dikenakan fungsi unsqueeze\\n  7. output encoder dikenakan fungsi unsqueeze\\n  8. hasil (6) dan (7) dikenakan fungsi bmm\\n  9. embedded[0] dan hasil (8) dikonkatenasi\\n  10. hasil (9) masuk ke linear layer\\n  11. hasil (10) dikenakan fungsi unsqueeze\\n  12. hasil (11) dikenakan fungsi relu\\n  13. hasil (12) masuk ke layer GRU\\n  14. hasil (13) masuk ke linear layer\\n  15. hasil (14) dikenakan fungsi log_softmax\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\"\"\"\n",
        "=== DECODER ===\n",
        "Menggunakan mekanisme atensi\n",
        "Terdiri dari:\n",
        "- 1 embedding layer\n",
        "- 3 linear layer\n",
        "- 1 dropout layer\n",
        "- 1 gru layer\n",
        "\n",
        "Forward:\n",
        "  1. input masuk ke embedding layer\n",
        "  2. hasil (1) diubah jadi berdimensi (1x1x_)\n",
        "  3. hasil (2) masuk ke dropout layer\n",
        "  3. embedded[0] dan hidden[0] dikonkatenasi\n",
        "  4. hasil (3) dimasukkan ke linear layer\n",
        "  5. hasil (4) diambil softmax --> jadi attention weight \n",
        "  6. hasil (5) dikenakan fungsi unsqueeze\n",
        "  7. output encoder dikenakan fungsi unsqueeze\n",
        "  8. hasil (6) dan (7) dikenakan fungsi bmm\n",
        "  9. embedded[0] dan hasil (8) dikonkatenasi\n",
        "  10. hasil (9) masuk ke linear layer\n",
        "  11. hasil (10) dikenakan fungsi unsqueeze\n",
        "  12. hasil (11) dikenakan fungsi relu\n",
        "  13. hasil (12) masuk ke layer GRU\n",
        "  14. hasil (13) masuk ke linear layer\n",
        "  15. hasil (14) dikenakan fungsi log_softmax\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxj6ozMPG0D3"
      },
      "source": [
        "# TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOtDvRV8GyQm"
      },
      "outputs": [],
      "source": [
        "# preparing training data\n",
        "\n",
        "# mengambil indeks dari setiap kata di sentence --> hasilnya list of indeks kata\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index else 0 for word in sentence.split(' ')]\n",
        "\n",
        "# mengembalikan tensor dengan elemennya adalah list of indeks kata yang dikonkatenasi dengan EOS_TOKEN (1)\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# mengembalikan tuple (input_tensor, target_tensor)\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzJteTQUG7M0"
      },
      "outputs": [],
      "source": [
        "# program for training\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    # inisialisasi hidden state encoder dengan zeros(1x1xhidden_size)\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    # set gradien semua tensor yang dioptimasi jadi nol : supaya tidak terakumulasi dengan gradien yang sudah ada\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # hitung panjang input dan output\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # inisialisasi encoder_outputs dengan zeros(max_length x hidden_size)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    # inisialisasi nilai loss = 0\n",
        "    loss = 0\n",
        "\n",
        "    # untuk setiap input lakukan:\n",
        "    for ei in range(input_length):\n",
        "        # forward encoder\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        # update nilai encoder_outputs\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    # inisialisasi input decoder dengan tensor [[0]]\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    # inisialisasi nilai hidden state decoder menjadi sama dengan hidden state encoder\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # dengan peluang teacher_forcing_ratio, tentukan apakah akan memakai teacher forcing atau tidak\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # jika memakai teacher forcing: \n",
        "    if use_teacher_forcing:\n",
        "        # untuk setiap output lakukan:\n",
        "        for di in range(target_length):\n",
        "            # forward decoder\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # update loss\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            # teacher forcing: Feed the target as the next input\n",
        "            decoder_input = target_tensor[di]  \n",
        "\n",
        "    # jika tidak menggunakan teacher forcing: use its own predictions as the next input\n",
        "    else:\n",
        "        # untuk setiap output lakukan:\n",
        "        for di in range(target_length):\n",
        "            # forward decoder\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # ambil nilai decoder_output terbesar\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            # lepaskan dari histori sebagai input\n",
        "            decoder_input = topi.squeeze().detach()  \n",
        "            # update loss\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            # berhenti jika sudah menemukan token EOS\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "    # lakukan backpropagation\n",
        "    loss.backward()\n",
        "    \n",
        "    # inisiasi optimizer\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj5c538mHEew"
      },
      "outputs": [],
      "source": [
        "# helper\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYlYpgIEHISo"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    # hitung waktu mulai\n",
        "    start = time.time()\n",
        "\n",
        "    plot_losses = []\n",
        "\n",
        "    # Reset every print_every\n",
        "    print_loss_total = 0 \n",
        "    # Reset every plot_every \n",
        "    plot_loss_total = 0  \n",
        "\n",
        "    # encoder menggunakan optimizer SGD (stochastic gradient descent)\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    # decoder menggunakan optimizer SGD (stochastic gradient descent)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # mengambil list of pasangan di train data secara acak sebanyak n_iters\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_data))\n",
        "                      for i in range(n_iters)]\n",
        "\n",
        "    # menggunakan loss negative log likelihood\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # untuk setiap pasangan di training_pairs\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        # lakukan training\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        \n",
        "        # update nilai loss total\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        # untuk menampilkan : jika sudah iterasi kelipatan print_every \n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            perc = iter / n_iters * 100\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, perc , print_loss_avg))\n",
        "            # save setiap iterasi kelipatan print_every\n",
        "            torch.save(encoder1.state_dict(), f\"{trained_model_encoder_path}_{perc}.pt\")\n",
        "            torch.save(attn_decoder1.state_dict(), f\"{trained_model_decoder_path}_{perc}.pt\")\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW4w4_HDHE_p"
      },
      "source": [
        "## TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBp8MsAZHNut",
        "outputId": "4b50ce3b-504a-4fef-ea9a-783b76959715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Time = 00:31:52\n",
            "18m 5s (- 343m 39s) (2500 5%) 5.1786\n",
            "37m 1s (- 333m 13s) (5000 10%) 5.0110\n",
            "56m 14s (- 318m 40s) (7500 15%) 4.9168\n",
            "74m 53s (- 299m 33s) (10000 20%) 4.8180\n",
            "93m 43s (- 281m 10s) (12500 25%) 4.7700\n",
            "112m 53s (- 263m 24s) (15000 30%) 4.6974\n",
            "132m 50s (- 246m 42s) (17500 35%) 4.6321\n",
            "150m 52s (- 226m 18s) (20000 40%) 4.5590\n",
            "170m 12s (- 208m 1s) (22500 45%) 4.5160\n",
            "188m 33s (- 188m 33s) (25000 50%) 4.4032\n",
            "206m 43s (- 169m 7s) (27500 55%) 4.4265\n",
            "224m 52s (- 149m 54s) (30000 60%) 4.3667\n",
            "242m 55s (- 130m 48s) (32500 65%) 4.3338\n",
            "261m 28s (- 112m 3s) (35000 70%) 4.3451\n",
            "279m 47s (- 93m 15s) (37500 75%) 4.2648\n",
            "297m 56s (- 74m 29s) (40000 80%) 4.1958\n",
            "316m 6s (- 55m 46s) (42500 85%) 4.1996\n",
            "335m 17s (- 37m 15s) (45000 90%) 4.1624\n",
            "354m 49s (- 18m 40s) (47500 95%) 4.1261\n",
            "372m 35s (- 0m 0s) (50000 100%) 4.1754\n"
          ]
        }
      ],
      "source": [
        "# current time\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print(\"Start Time =\", current_time)\n",
        "\n",
        "# train\n",
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 50000, print_every=2500)\n",
        "#trainIters(encoder1, attn_decoder1, 2, print_every=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4vEfk8BKsOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2221a08c-bd0e-49c1-f77a-c2ca4e9bd27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish Time = 06:44:29\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "finish = datetime.now()\n",
        "\n",
        "finish_time = finish.strftime(\"%H:%M:%S\")\n",
        "print(\"Finish Time =\", finish_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_N1RapUG-eB"
      },
      "outputs": [],
      "source": [
        "# save trained model - state dict\n",
        "torch.save(encoder1.state_dict(), f\"{trained_model_encoder_path}.pt\")\n",
        "torch.save(attn_decoder1.state_dict(), f\"{trained_model_decoder_path}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD TRAINED MODEL"
      ],
      "metadata": {
        "id": "KVi-V3jszCwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# redefine the model\n",
        "hidden_size = 256\n",
        "encoder_trained = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder_trained = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "# load encoder trained model\n",
        "encoder_trained.load_state_dict(torch.load(f\"{trained_model_encoder_path}.pt\"))\n",
        "encoder_trained.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0mgAx8qzEWH",
        "outputId": "960ab0d7-b9e2-4870-fc86-3c99080fe357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderRNN(\n",
              "  (embedding): Embedding(17725, 256)\n",
              "  (gru): GRU(256, 256)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load decoder trained model\n",
        "attn_decoder_trained.load_state_dict(torch.load(f\"{trained_model_decoder_path}.pt\"))\n",
        "attn_decoder_trained.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3olx8xO1fIo",
        "outputId": "4d152610-3188-48c1-866f-cd66672dd23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttnDecoderRNN(\n",
              "  (embedding): Embedding(16806, 256)\n",
              "  (attn): Linear(in_features=512, out_features=15, bias=True)\n",
              "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (gru): GRU(256, 256)\n",
              "  (out): Linear(in_features=256, out_features=16806, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATE THE MODEL"
      ],
      "metadata": {
        "id": "tkWhJpoc2DFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load validation data\n",
        "# load + normalize train data\n",
        "val_data = load_data(val_data_dir)\n",
        "\n",
        "# check how many sentence pairs\n",
        "print(\"Read %s sentence pairs\" % len(val_data))\n",
        "\n",
        "# take only data train with len < 20\n",
        "val_data = filterPairs(val_data)\n",
        "print(\"Trimmed to %s sentence pairs\" % len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqXLaShrQJz9",
        "outputId": "ce34103f-ea70-4606-b0a3-93c333142dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 2677 sentence pairs\n",
            "Trimmed to 1304 sentence pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "id": "LIY7gbZJ1iTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, encoder, decoder, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = datum[0]\n",
        "        trg = [o for o in datum[1].split(' ')]\n",
        "        \n",
        "        output_words, attentions = evaluate(encoder, decoder, src)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = output_words[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "metadata": {
        "id": "Z7W9z1jsQ_mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scr = calculate_bleu(val_data, encoder_trained, attn_decoder_trained, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_scr*100:.10f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PvcJe-HSRHX",
        "outputId": "12dfcca5-b9f4-4267-c262-04d3778eb275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 5.2515426441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(val_data)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "r05QgcUh3Ljs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder_trained, attn_decoder_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yMGMo5D3Pls",
        "outputId": "5a857595-bb92-4663-8bcb-d9e43b0a298f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> that is me playing my imaginary piano .\n",
            "= itu adalah saya memainkan piano khayalan saya .\n",
            "< itu adalah saya saya saya . <EOS>\n",
            "\n",
            "> she read two sentences .\n",
            "= dia membaca dua kalimat .\n",
            "< dia ia dua dua . . . <EOS>\n",
            "\n",
            "> somebody who got educated here .\n",
            "= yang mengenyam pendidikan di sana .\n",
            "< yang yang ada di sini . <EOS>\n",
            "\n",
            "> this is rising . comfort stays whole .\n",
            "= ini semakin naik . kenyamanannya tetap .\n",
            "< ini ini bukan . . <EOS>\n",
            "\n",
            "> because of the hunger i was forced to drop out of school .\n",
            "= karena kelaparan saya terpaksa putus sekolah .\n",
            "< karena anak saya saya saya dari . . <EOS>\n",
            "\n",
            "> and the truth is this guy can probably explain this to you .\n",
            "= sebenarnya orang ini dapat menjelaskannya .\n",
            "< dan dan adalah adalah adalah untuk anda dapat untuk . <EOS>\n",
            "\n",
            "> and by that time the women are yelling and screaming inside the car .\n",
            "= waktu itu semua perempuan berteriak di dalam mobil .\n",
            "< dan saat ini membawa oleh wanita dan dan telah menjadi . <EOS>\n",
            "\n",
            "> it gave me confidence . it gave me a career .\n",
            "= memberi saya rasa percaya diri dan karir .\n",
            "< ini ini saya saya saya . . <EOS>\n",
            "\n",
            "> that s twice as long as humans have been on this planet .\n",
            "= itu dua kali lebih lama daripada sejarah manusia di planet ini .\n",
            "< itulah yang yang telah menjadi di ini . <EOS>\n",
            "\n",
            "> i grew up in a very sheltered environment .\n",
            "= saya tumbuh di tengah lingkungan yang cukup terlindung .\n",
            "< saya berada di dalam di di . <EOS>\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}